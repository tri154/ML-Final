{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"},{"sourceId":10238128,"sourceType":"datasetVersion","datasetId":6303980},{"sourceId":203983,"sourceType":"modelInstanceVersion","modelInstanceId":172924,"modelId":195260}],"dockerImageVersionId":30823,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport copy\nimport pickle\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport polars as pl\nimport polars.selectors as cs\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.pipeline import Pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:44:28.874256Z","iopub.execute_input":"2024-12-22T03:44:28.874581Z","iopub.status.idle":"2024-12-22T03:44:57.031802Z","shell.execute_reply.started":"2024-12-22T03:44:28.874524Z","shell.execute_reply":"2024-12-22T03:44:57.031087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv(\"/kaggle/input/child-mind-institute-problematic-internet-use/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/child-mind-institute-problematic-internet-use/test.csv\")\ndictionary = pd.read_csv(\"/kaggle/input/child-mind-institute-problematic-internet-use/data_dictionary.csv\")\n# sample_submission = pd.read_csv(\"/kaggle/input/dataset/sample_submission.csv\")\n\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:44:57.032556Z","iopub.execute_input":"2024-12-22T03:44:57.033172Z","iopub.status.idle":"2024-12-22T03:44:57.155158Z","shell.execute_reply.started":"2024-12-22T03:44:57.033149Z","shell.execute_reply":"2024-12-22T03:44:57.154405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"id_to_test = np.array(test['id'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:44:57.155958Z","iopub.execute_input":"2024-12-22T03:44:57.156234Z","iopub.status.idle":"2024-12-22T03:44:57.160282Z","shell.execute_reply.started":"2024-12-22T03:44:57.15621Z","shell.execute_reply":"2024-12-22T03:44:57.159414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    df = df.loc[:, ['X', 'Y', 'Z', 'enmo', 'anglez']]\n    return df.describe().values.reshape(-1), filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    \n    stats, indexes = zip(*results)\n    \n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    return df\n\n\nclass AutoEncoder(nn.Module):\n    def __init__(self, input_dim, encoding_dim):\n        super(AutoEncoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, encoding_dim*3),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*3, encoding_dim*2),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*2, encoding_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(encoding_dim, input_dim*2),\n            nn.ReLU(),\n            nn.Linear(input_dim*2, input_dim*3),\n            nn.ReLU(),\n            nn.Linear(input_dim*3, input_dim),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\n\ndef perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(df)\n    \n    data_tensor = torch.FloatTensor(df_scaled)\n    \n    input_dim = data_tensor.shape[1]\n    autoencoder = AutoEncoder(input_dim, encoding_dim)\n    \n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(autoencoder.parameters())\n    \n    for epoch in range(epochs):\n        for i in range(0, len(data_tensor), batch_size):\n            batch = data_tensor[i : i + batch_size]\n            optimizer.zero_grad()\n            reconstructed = autoencoder(batch)\n            loss = criterion(reconstructed, batch)\n            loss.backward()\n            optimizer.step()\n            \n        if (epoch + 1) % 10 == 0:\n            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n                 \n    with torch.no_grad():\n        encoded_data = autoencoder.encoder(data_tensor).numpy()\n        \n    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n    \n    return df_encoded\n\ndef feature_engineering(df):\n    season_cols = [col for col in df.columns if 'Season' in col]\n    df = df.drop(season_cols, axis=1) \n    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n    df['BMI_PHR'] = df['Physical-BMI'] * df['Physical-HeartRate']\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:44:57.161282Z","iopub.execute_input":"2024-12-22T03:44:57.161579Z","iopub.status.idle":"2024-12-22T03:44:57.180392Z","shell.execute_reply.started":"2024-12-22T03:44:57.16155Z","shell.execute_reply":"2024-12-22T03:44:57.179706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os \nfrom tqdm import tqdm \ndef extract_enmo(df_source, id=None):\n    df = df_source.copy()\n    df = df[df['non-wear_flag'] == 0]\n    df.drop('non-wear_flag', axis=1, inplace=True)\n    df.loc[:, 'Type_activity'] = 'Non-assigned'\n    df.loc[(df['enmo'] < 10*1e-3), 'Type_activity'] = 'sedentary'\n    df.loc[(df['enmo'] >= 10*1e-3) & (df['enmo'] < 100*1e-3), 'Type_activity'] = 'light'\n    df.loc[(df['enmo'] >= 100*1e-3), 'Type_activity'] = 'moderate'\n    \n    total_wear = df['step'].count()\n    \n    sedentary_perall = df[df['Type_activity'] == 'sedentary']['step'].count()\n    sedentary_perall = sedentary_perall / total_wear\n    \n    light_perall = df[df['Type_activity'] == 'light']['step'].count()\n    light_perall = light_perall / total_wear\n    \n    moderate_perall = df[df['Type_activity'] == 'moderate']['step'].count()\n    moderate_perall = moderate_perall / total_wear\n\n    sedentary_perall, light_perall, moderate_perall\n    return pd.DataFrame({'id': [id], \n                         'sedentary_por': [sedentary_perall], \n                         'light_por': [light_perall],\n                         'moderate_por': [moderate_perall]}\n                       )\n\ndef getEnmo(ts_path):\n    listdir = os.listdir(ts_path)\n    res_df = None\n    for dir in tqdm(listdir):\n        # print(dir)\n        dft = pd.read_parquet(os.path.join(ts_path, dir, \"part-0.parquet\"))\n        \n        id = dir[3:]\n        ex_df = extract_enmo(dft, id=id)\n        if res_df is None:\n            res_df = ex_df\n        else:\n            res_df = pd.concat([res_df, ex_df])\n    return res_df\n\n#res_df là kết quả","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:44:57.181151Z","iopub.execute_input":"2024-12-22T03:44:57.181388Z","iopub.status.idle":"2024-12-22T03:44:57.203421Z","shell.execute_reply.started":"2024-12-22T03:44:57.181368Z","shell.execute_reply":"2024-12-22T03:44:57.202824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\nfrom torch.utils.data import DataLoader\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom sklearn.preprocessing import StandardScaler\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.cur_epoches = 0\n        #input (N, 5)\n        self.conv1 = nn.Conv1d(5, 64, kernel_size=3, stride=2, padding='valid') # 32, N, 1\n        self.avgpool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=2, padding='valid') # 32, N, 1\n        self.avgpool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        self.conv3 = nn.Conv1d(128, 128, kernel_size=3, stride=2, padding='valid') # 32, N, 1\n        self.avgpool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, stride=2, padding='valid') # 32, N, 1\n        self.avgpool4 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        self.conv5 = nn.Conv1d(256, 256, kernel_size=3, stride=2, padding='valid')\n        \n        self.fc1 = nn.Linear(256, 128) # Adjust output size based on input dims\n        self.fc15 = nn.Linear(128, 64)\n        self.fc2 = nn.Linear(64, 4)\n\n        self.dropout1 = nn.Dropout(0.3, inplace=False)\n        self.dropout2 = nn.Dropout(0.3, inplace=False)\n\n        self.act = nn.LeakyReLU(0.1)\n        self.act1 = nn.Sigmoid()\n\n    def forward(self, x, debug=False):\n        x = self.act(self.conv1(x))\n        if debug: print(x.shape)\n        x = self.avgpool1(x)\n        if debug: print(x.shape)\n\n        x = self.act(self.conv2(x))\n        if debug: print(x.shape)\n        x = self.avgpool2(x)\n        if debug: print(x.shape)\n\n        x = self.act(self.conv3(x))\n        if debug: print(x.shape)\n        x = self.avgpool3(x)\n        if debug: print(x.shape)\n\n        x = self.act(self.conv4(x))\n        if debug: print(x.shape)\n        x = self.avgpool4(x)\n        if debug: print(x.shape)\n\n        x = self.act(self.conv5(x))\n        if debug: print(x.shape)\n\n        x = F.adaptive_avg_pool1d(x, 1).squeeze()\n        if debug: print(x.shape)\n\n        x = self.act1(self.fc1(x))\n        if debug: print(x.shape)\n        x = self.dropout1(x)\n        \n        x = self.act1(self.fc15(x))\n        x = self.dropout2(x)\n        \n        x = self.fc2(x)\n        if debug: print(x.shape)\n        x = F.softmax(x, dim=0)\n\n        return x\n\n    def feature_extract(self, x, debug=False):\n        x = self.act(self.conv1(x))\n        if debug: print(x.shape)\n        x = self.avgpool1(x)\n        if debug: print(x.shape)\n\n        x = self.act(self.conv2(x))\n        if debug: print(x.shape)\n        x = self.avgpool2(x)\n        if debug: print(x.shape)\n\n        x = self.act(self.conv3(x))\n        if debug: print(x.shape)\n        x = self.avgpool3(x)\n        if debug: print(x.shape)\n\n        x = self.act(self.conv4(x))\n        if debug: print(x.shape)\n        x = self.avgpool4(x)\n        if debug: print(x.shape)\n\n        x = self.act(self.conv5(x))\n        if debug: print(x.shape)\n\n        x = F.adaptive_avg_pool1d(x, 1).squeeze()\n        if debug: print(x.shape)\n\n        x = self.act1(self.fc1(x))\n        if debug: print(x.shape)\n        x = self.dropout1(x)\n        \n        x = self.act1(self.fc15(x)) #64\n        y = self.dropout2(x)\n        \n        y = self.fc2(y)\n        if debug: print(y.shape)\n        y = F.softmax(y, dim=0) #4\n\n        return x, y\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:44:57.205736Z","iopub.execute_input":"2024-12-22T03:44:57.206066Z","iopub.status.idle":"2024-12-22T03:44:57.225227Z","shell.execute_reply.started":"2024-12-22T03:44:57.206035Z","shell.execute_reply":"2024-12-22T03:44:57.224551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, device, path_tabu, path_ts, preload=True, type='train'):\n        self.device = device\n        self.path_ts = path_ts\n        self.tabu_data = pd.read_csv(path_tabu) \n        self.ids = [x[3:] for x in os.listdir(path_ts)]\n        self.filter()\n        self.ids.sort()\n        self.type = type\n        if type == 'train':\n            n = len(self.ids)\n            self.ids = self.ids[:int(n*0.8)]\n        elif type == 'val':\n            n = len(self.ids)\n            self.ids = self.ids[int(n*0.8):]\n        self.preload = preload\n        if self.preload:\n            self.ts_data_X, self.ts_data_Y = self.load_all_data()\n    def filter(self):\n        temp_ids = []\n        for id in tqdm(self.ids):\n            df = pd.read_parquet(os.path.join(self.path_ts, \"id=\" + id, \"part-0.parquet\"))\n            if df.shape[0] >= 900:\n                temp_ids.append(id)\n        self.ids = temp_ids\n\n    def collate(self, index):\n        X = [self.ts_data_X[i].to(self.device) for i in index]\n        Y = None\n        if self.ts_data_Y is not None:\n            Y = [self.ts_data_Y[i] for i in index]\n            Y = torch.tensor(Y, dtype=torch.int64)\n            Y = torch.nn.functional.one_hot(Y, num_classes=4).to(self.device).to(torch.float32)\n        return {'X': X,\n                'Y': Y}\n            \n    def dataloader(self, batch_size=1):\n        size = len(self.ts_data_X)\n        batch_size = size if batch_size == -1 else batch_size\n        loader = DataLoader(list(range(size)), batch_size=batch_size, collate_fn=self.collate, shuffle=True if self.type == 'test' else False, num_workers=0)\n        return loader\n        \n    def load_all_data(self):\n        count = 0\n        inputs = []\n        labels = None if self.type == 'test' else list()\n        for id in tqdm(self.ids):\n            df = pd.read_parquet(os.path.join(self.path_ts, \"id=\" + id, \"part-0.parquet\"))\n            df = df.loc[:, ['X', 'Y', 'Z', 'enmo', 'anglez']]\n            # normalize the signals \n            scaler = StandardScaler()\n            df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n            input = torch.tensor(df.values)\n            input = input.T\n            inputs.append(input)\n            if self.type != 'test': labels.append(self.tabu_data[self.tabu_data['id'] == id]['sii'].values[0])\n\n            \n            # count += 1\n            # if count == 2:\n            #     break\n        \n        return inputs, labels\n        \n# train_tabu_path=\"/kaggle/input/child-mind-institute-problematic-internet-use/train.csv\"\n# train_ts_path='/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet'\n# dataset = Dataset('cpu', path_tabu=train_tabu_path, path_ts=train_ts_path, type='train')\n# dataloader = dataset.dataloader()\n# model.zero_grad()\n# for data in dataloader:\n#     print(data['X'][0])\n#     print(data['Y'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:44:57.226643Z","iopub.execute_input":"2024-12-22T03:44:57.226868Z","iopub.status.idle":"2024-12-22T03:44:57.248359Z","shell.execute_reply.started":"2024-12-22T03:44:57.226849Z","shell.execute_reply":"2024-12-22T03:44:57.247661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_feature_64_4(tabu_path, ts_path, merge=False):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_path = \"/kaggle/input/cnn-feature_extractor/pytorch/default/5/model-50 (1).pth\"\n    model = CNN()\n    model.load_state_dict(torch.load(model_path, weights_only=True))\n    model.to(device)\n    model.eval()\n\n    trainset = Dataset(device, path_tabu=tabu_path, path_ts=ts_path, type='test')\n    dataloader = trainset.dataloader(batch_size=1)\n\n\n    features64 = list()\n    features4 = list()\n    with torch.no_grad():\n        for data in dataloader:\n            X = data['X'][0]\n            x_64, x_4 = model.feature_extract(X)\n            features64.append(x_64)\n            features4.append(x_4)\n    \n    features64 = torch.stack(features64, dim=0)\n    features4 = torch.stack(features4, dim=0)\n    features64 = features64.cpu().detach().numpy()\n    features4 = features4.cpu().detach().numpy()\n    ids = np.array(trainset.ids)\n    if merge:\n        features68 = np.concatenate([features64, features4], axis=-1)\n        ts_features_df = pd.DataFrame(features68, columns=[f'feature_{i}' for i in range(features68.shape[1])])\n        ts_features_df.insert(0, 'id', ids)\n        return ts_features_df\n    else:\n        ts_features_df64 = pd.DataFrame(features64, columns=[f'feature64_{i}' for i in range(features64.shape[1])])\n        ts_features_df64.insert(0, 'id', ids)\n\n        ts_features_df4 = pd.DataFrame(features4, columns=[f'feature4_{i}' for i in range(features4.shape[1])])\n        ts_features_df4.insert(0, 'id', ids)\n        return ts_features_df64, ts_features_df4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:44:57.248968Z","iopub.execute_input":"2024-12-22T03:44:57.249217Z","iopub.status.idle":"2024-12-22T03:44:57.273953Z","shell.execute_reply.started":"2024-12-22T03:44:57.249197Z","shell.execute_reply":"2024-12-22T03:44:57.27314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tabu_path_train = '/kaggle/input/child-mind-institute-problematic-internet-use/train.csv'\nts_path_train = '/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet'\n\ntabu_path_test = '/kaggle/input/child-mind-institute-problematic-internet-use/test.csv'\nts_path_test = '/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet'\n\ntrain_64, train_4 = get_feature_64_4(tabu_path_train, ts_path_train, merge=False)\ntest_64, test_4 = get_feature_64_4(tabu_path_test, ts_path_test, merge=False)\n\ntrain_enmo = getEnmo(ts_path_train)\ntest_enmo = getEnmo(ts_path_test)\n\ntrain_ts_7 = train_enmo.merge(train_4, on ='id', how='left')\ntest_ts_7 = test_enmo.merge(test_4, on='id', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:44:57.274716Z","iopub.execute_input":"2024-12-22T03:44:57.274934Z","iopub.status.idle":"2024-12-22T03:48:24.686116Z","shell.execute_reply.started":"2024-12-22T03:44:57.274916Z","shell.execute_reply":"2024-12-22T03:48:24.685261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ndf_train = train_ts.drop('id', axis=1)\ndf_test = test_ts.drop('id', axis=1)\n\ntrain_ts_encoded = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\ntest_ts_encoded = perform_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n\ntrain_ts_encoded[\"id\"]=train_ts[\"id\"]\ntest_ts_encoded['id']=test_ts[\"id\"]\n\ntrain_ts_encoded = train_ts_encoded.merge(train_ts_7, on='id', how='left')\ntest_ts_encoded = test_ts_encoded.merge(test_ts_7, on='id', how='left')\n\n#train_ts_encoded, test_ts_encoded là feature cuối.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:48:24.686909Z","iopub.execute_input":"2024-12-22T03:48:24.687121Z","iopub.status.idle":"2024-12-22T03:49:14.297976Z","shell.execute_reply.started":"2024-12-22T03:48:24.687102Z","shell.execute_reply":"2024-12-22T03:49:14.296903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = train.merge(train_ts_encoded, on='id', how='left')\ntest = test.merge(test_ts_encoded, on='id', how='left')\ntrain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:49:14.299098Z","iopub.execute_input":"2024-12-22T03:49:14.299917Z","iopub.status.idle":"2024-12-22T03:49:14.335548Z","shell.execute_reply.started":"2024-12-22T03:49:14.299884Z","shell.execute_reply":"2024-12-22T03:49:14.334647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn import preprocessing\n\nencoding = preprocessing.LabelEncoder()\n\ndef convert(dataset):\n    if 'id' in dataset.columns:\n        getId = dataset['id']\n        dataset = dataset.drop(columns = 'id')\n    get_columns = dataset.select_dtypes(exclude=\"number\").columns\n    for cols in get_columns:\n        dataset[cols] = encoding.fit_transform(dataset[cols])\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:49:14.336397Z","iopub.execute_input":"2024-12-22T03:49:14.336694Z","iopub.status.idle":"2024-12-22T03:49:14.341204Z","shell.execute_reply.started":"2024-12-22T03:49:14.336663Z","shell.execute_reply":"2024-12-22T03:49:14.340545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reduction(dataset):\n    for cols in dataset.columns:\n        if (cols != 'sii') and (dataset[cols].isna().sum() > len(dataset[cols]) * 0.7):\n            dataset.drop(cols, axis=1, errors=\"ignore\", inplace=True)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:49:14.342071Z","iopub.execute_input":"2024-12-22T03:49:14.342365Z","iopub.status.idle":"2024-12-22T03:49:14.358849Z","shell.execute_reply.started":"2024-12-22T03:49:14.342343Z","shell.execute_reply":"2024-12-22T03:49:14.358141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fillCategory(dataset):\n    cols = dataset.select_dtypes(\"object\").columns\n    dataset[cols] = dataset[cols].fillna(\"Missing\")\n    dataset[cols] = dataset[cols].astype(\"category\")\n    return dataset\n\n#def fillNum(dataset):\n    #cols = dataset.select_dtypes(\"number\").columns\n    #dataset[cols] = dataset[cols].fillna(dataset[cols].median())\n#    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:49:14.381412Z","iopub.execute_input":"2024-12-22T03:49:14.381641Z","iopub.status.idle":"2024-12-22T03:49:14.397675Z","shell.execute_reply.started":"2024-12-22T03:49:14.381622Z","shell.execute_reply":"2024-12-22T03:49:14.396912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"common_columns = train.columns.intersection(test.columns)\nfinal_train = pd.DataFrame(train[common_columns])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:49:14.398569Z","iopub.execute_input":"2024-12-22T03:49:14.398889Z","iopub.status.idle":"2024-12-22T03:49:14.415675Z","shell.execute_reply.started":"2024-12-22T03:49:14.398839Z","shell.execute_reply":"2024-12-22T03:49:14.41481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_train['sii'] = train['sii'].reset_index(drop=True)\nfinal_train = final_train.dropna(subset='sii')\n\nfinal_train = reduction(final_train)\nfinal_train = fillCategory(final_train)\nfinal_train = convert(final_train)\ntest = convert(test)\nfinal_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:49:14.416461Z","iopub.execute_input":"2024-12-22T03:49:14.416652Z","iopub.status.idle":"2024-12-22T03:49:14.502565Z","shell.execute_reply.started":"2024-12-22T03:49:14.416633Z","shell.execute_reply":"2024-12-22T03:49:14.501848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:49:14.503393Z","iopub.execute_input":"2024-12-22T03:49:14.503646Z","iopub.status.idle":"2024-12-22T03:49:14.529936Z","shell.execute_reply.started":"2024-12-22T03:49:14.503616Z","shell.execute_reply":"2024-12-22T03:49:14.529037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#final_train = fillCategory(final_train)\n#final_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:49:14.53261Z","iopub.execute_input":"2024-12-22T03:49:14.53286Z","iopub.status.idle":"2024-12-22T03:49:14.546066Z","shell.execute_reply.started":"2024-12-22T03:49:14.53284Z","shell.execute_reply":"2024-12-22T03:49:14.545231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_common_columns = test.columns.intersection(final_train.columns)\nfinal_test = pd.DataFrame(test[new_common_columns])\nfinal_test = convert(final_test)\nlen(final_test.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:49:14.547341Z","iopub.execute_input":"2024-12-22T03:49:14.547581Z","iopub.status.idle":"2024-12-22T03:49:14.565859Z","shell.execute_reply.started":"2024-12-22T03:49:14.54756Z","shell.execute_reply":"2024-12-22T03:49:14.565073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rounding(y):\n    return np.where(y < 0.5, 0,\n                    np.where(y < 1.5, 1,\n                             np.where(y < 2.5, 2, 3)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:49:14.359738Z","iopub.execute_input":"2024-12-22T03:49:14.360043Z","iopub.status.idle":"2024-12-22T03:49:14.380554Z","shell.execute_reply.started":"2024-12-22T03:49:14.360015Z","shell.execute_reply":"2024-12-22T03:49:14.379829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfeature_train = final_train.drop(columns = 'sii')\nX = np.array(feature_train)\ny = np.array(final_train[\"sii\"])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n\nX_to_test = np.array(final_test)\ny_to_test = y[:20]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:49:14.566647Z","iopub.execute_input":"2024-12-22T03:49:14.566904Z","iopub.status.idle":"2024-12-22T03:49:14.587247Z","shell.execute_reply.started":"2024-12-22T03:49:14.566881Z","shell.execute_reply":"2024-12-22T03:49:14.586496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nparam_distributions = {\n    'iterations': [1, 5, 10, 50, 100, 200],\n    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.5, 0.8],\n    'depth': [1, 2, 3, 5, 10, 15],\n}\nclassifier = CatBoostRegressor(verbose=0, task_type = 'GPU', gpu_ram_part = 0.5, early_stopping_rounds=20)\n\nCatBoost = RandomizedSearchCV(\n    estimator=classifier,\n    param_distributions=param_distributions,\n    n_iter=100, \n    cv=3,\n    scoring='r2', \n    random_state=42,\n    n_jobs=1\n)\nCatBoost.fit(X_train, y_train, use_best_model=False)\nprediction1 = CatBoost.predict(X_to_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:49:14.588087Z","iopub.execute_input":"2024-12-22T03:49:14.588307Z","iopub.status.idle":"2024-12-22T04:05:24.253553Z","shell.execute_reply.started":"2024-12-22T03:49:14.588286Z","shell.execute_reply":"2024-12-22T04:05:24.252607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction1 = prediction1.flatten()\nprediction1 = rounding(prediction1)\nprediction1.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T04:05:24.254549Z","iopub.execute_input":"2024-12-22T04:05:24.254844Z","iopub.status.idle":"2024-12-22T04:05:24.260814Z","shell.execute_reply.started":"2024-12-22T04:05:24.254819Z","shell.execute_reply":"2024-12-22T04:05:24.260124Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nparam_distributions = {\n    \n    \"n_estimators\": [5, 10, 25, 50, 100, 200],\n    \"learning_rate\": [0.005, 0.01, 0.05, 0.1, 0.5, 0.8],\n    \"lambda_l1\": [0.1, 1, 5, 10, 25, 50],\n    \"lambda_l2\": [0.1, 1, 5, 10, 25, 50],\n}\nclassifier = lgb.LGBMClassifier(verboss=0, force_col_wise=True)\n\nLightGBM = RandomizedSearchCV(\n    estimator=classifier,\n    param_distributions=param_distributions,\n    n_iter=100,\n    cv=3,\n    scoring='r2', \n    random_state=42,\n    n_jobs=-1\n)\n\nLightGBM.fit(X_train, y_train)\nprediction2 = LightGBM.predict(X_to_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T04:27:11.58035Z","iopub.execute_input":"2024-12-22T04:27:11.580696Z","iopub.status.idle":"2024-12-22T04:31:13.245776Z","shell.execute_reply.started":"2024-12-22T04:27:11.580668Z","shell.execute_reply":"2024-12-22T04:31:13.244792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction2 = rounding(prediction2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T04:31:13.246978Z","iopub.execute_input":"2024-12-22T04:31:13.247238Z","iopub.status.idle":"2024-12-22T04:31:13.25147Z","shell.execute_reply.started":"2024-12-22T04:31:13.247216Z","shell.execute_reply":"2024-12-22T04:31:13.250387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\n\nfrom sklearn.metrics import cohen_kappa_score\nparam_distributions = {\n    \"n_estimators\": [5, 10, 25, 50, 100, 500],\n    \"max_depth\" : [1, 5, 10, 25, 50, 100],\n    \"learning_rate\": [0.005, 0.01, 0.05, 0.1, 0.5, 0.8],\n    \"min_child_weight\": [1, 5, 10, 20, 30, 50],\n    \"colsample_bytree\": [0.01, 0.05, 0.1, 0.5, 0.8, 1],\n    \"subsample\": [0.01, 0.05, 0.1, 0.5, 0.8, 1],\n    \"reg_alpha\": [0.1, 1, 5, 10, 25, 50],\n    \"reg_lambda\": [0.1, 1, 5, 10, 25, 50],\n}\nclassifier = XGBRegressor()\nXGBoost = RandomizedSearchCV (\n    estimator=classifier,\n    param_distributions=param_distributions,\n    n_iter=100, \n    cv=3,\n    scoring='r2', \n    random_state=42,\n    n_jobs=-1\n)\nXGBoost.fit(X_train, y_train)    \nprediction3 = XGBoost.predict(X_to_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T04:31:13.252982Z","iopub.execute_input":"2024-12-22T04:31:13.253219Z","iopub.status.idle":"2024-12-22T04:31:27.140626Z","shell.execute_reply.started":"2024-12-22T04:31:13.253198Z","shell.execute_reply":"2024-12-22T04:31:27.139721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction3 = rounding(prediction3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T04:31:27.141941Z","iopub.execute_input":"2024-12-22T04:31:27.142236Z","iopub.status.idle":"2024-12-22T04:31:27.146224Z","shell.execute_reply.started":"2024-12-22T04:31:27.142202Z","shell.execute_reply":"2024-12-22T04:31:27.14524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction1 = pd.Series(prediction1)\nprediction2 = pd.Series(prediction2)\nprediction3 = pd.Series(prediction3)\n#prediction = pd.DataFrame(prediction1, prediction2, prediction3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T05:13:14.554359Z","iopub.execute_input":"2024-12-22T05:13:14.554721Z","iopub.status.idle":"2024-12-22T05:13:14.559039Z","shell.execute_reply.started":"2024-12-22T05:13:14.554692Z","shell.execute_reply":"2024-12-22T05:13:14.558188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': id_to_test,\n    'sii': prediction1\n})\nsubmission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T06:02:52.617183Z","iopub.execute_input":"2024-12-22T06:02:52.617477Z","iopub.status.idle":"2024-12-22T06:02:52.626227Z","shell.execute_reply.started":"2024-12-22T06:02:52.617451Z","shell.execute_reply":"2024-12-22T06:02:52.625208Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T06:02:58.208647Z","iopub.execute_input":"2024-12-22T06:02:58.209102Z","iopub.status.idle":"2024-12-22T06:02:58.215416Z","shell.execute_reply.started":"2024-12-22T06:02:58.209064Z","shell.execute_reply":"2024-12-22T06:02:58.214432Z"}},"outputs":[],"execution_count":null}]}