{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da8e3f9",
   "metadata": {
    "papermill": {
     "duration": 0.01007,
     "end_time": "2024-12-19T21:26:12.591648",
     "exception": false,
     "start_time": "2024-12-19T21:26:12.581578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The baseline was taken from [CMI | Reproducible results |FixSeed,LGB-CPU|LB.494](https://www.kaggle.com/code/kuosys/cmi-reproducible-results-fixseed-lgb-cpu-lb-494) üôè"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22021b23",
   "metadata": {
    "papermill": {
     "duration": 0.008379,
     "end_time": "2024-12-19T21:26:12.608917",
     "exception": false,
     "start_time": "2024-12-19T21:26:12.600538",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Metric Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f775d7",
   "metadata": {
    "papermill": {
     "duration": 0.00822,
     "end_time": "2024-12-19T21:26:12.625552",
     "exception": false,
     "start_time": "2024-12-19T21:26:12.617332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.\n",
    "\n",
    "To compute the quadratic weighted kappa, we construct three matrices, $O$, $W$, and $E$, with $N$ the number of distinct labels.\n",
    "\n",
    "The matrix $O$ is an $N √ó ùëÅ$ histogram matrix such that $O_{i,j}$ corresponds to the number of instances that have an actual value $i$ and a predicted value $j$.\n",
    "\n",
    "The matrix $W$ is an $N √ó ùëÅ$ matrix of weights, calculated based on the squared difference between actual and predicted values:\n",
    "\n",
    "$$ W_{i,j} = \\frac{(i - j)^2}{(N - 1)^2}.$$\n",
    "\n",
    "The matrix $E$ is an $N √ó ùëÅ$ histogram matrix of expected outcomes, calculated assuming that there is no correlation between values. This is calculated as the outer product between the actual histogram vector of outcomes and the predicted histogram vector, normalized such that $E$ and $O$ have the same sum.\n",
    "\n",
    "From these three matrices, the quadratic weighted kappa is calculated as: \n",
    "\n",
    "$$ùúÖ = 1 - \\frac{\\sum_{i, j} W_{i, j} \\cdot O_{i, j}}{\\sum_{i, j} W_{i, j} \\cdot E_{i, j}}.$$\n",
    "\n",
    "This metric can be interpreted as one minus the ratio of the squared error on the predicted values ‚Äã‚Äãto the error if we had randomly assigned predictions from a class distribution given by the proportions of predicted values. To learn more about this metric, you can find detailed information online, for example at [https://datatab.net/tutorial/weighted-cohens-kappa](https://datatab.net/tutorial/weighted-cohens-kappa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24015f2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:26:12.644157Z",
     "iopub.status.busy": "2024-12-19T21:26:12.643835Z",
     "iopub.status.idle": "2024-12-19T21:26:29.227985Z",
     "shell.execute_reply": "2024-12-19T21:26:29.227281Z"
    },
    "papermill": {
     "duration": 16.595335,
     "end_time": "2024-12-19T21:26:29.229439",
     "exception": false,
     "start_time": "2024-12-19T21:26:12.634104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ae48ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:26:29.249034Z",
     "iopub.status.busy": "2024-12-19T21:26:29.248395Z",
     "iopub.status.idle": "2024-12-19T21:27:39.465127Z",
     "shell.execute_reply": "2024-12-19T21:27:39.464190Z"
    },
    "papermill": {
     "duration": 70.227709,
     "end_time": "2024-12-19T21:27:39.466713",
     "exception": false,
     "start_time": "2024-12-19T21:26:29.239004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 996/996 [01:09<00:00, 14.27it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 11.21it/s]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "        \n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)   \n",
    "\n",
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "        \n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train)\n",
    "    mappingTe = create_mapping(col, test)\n",
    "    \n",
    "    train[col] = train[col].replace(mapping).astype(int)\n",
    "    test[col] = test[col].replace(mappingTe).astype(int)\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def TrainML(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    print('OPTIMIZED THRESHOLDS', KappaOPtimizer.x)\n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "    optimized_thresholds = KappaOPtimizer.x\n",
    "    return submission, oof_tuned, oof_non_rounded, y, optimized_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fec400e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:27:39.511924Z",
     "iopub.status.busy": "2024-12-19T21:27:39.511621Z",
     "iopub.status.idle": "2024-12-19T21:28:01.768485Z",
     "shell.execute_reply": "2024-12-19T21:28:01.767732Z"
    },
    "papermill": {
     "duration": 22.28057,
     "end_time": "2024-12-19T21:28:01.769836",
     "exception": false,
     "start_time": "2024-12-19T21:27:39.489266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:22<00:00,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.8939\n",
      "Mean Validation QWK ---> 0.3897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZED THRESHOLDS [0.6264773  0.89171596 2.75349008]\n",
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.445\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "n_splits = 5\n",
    "\n",
    "model = XGBRegressor(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    n_estimators=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree = 0.8,\n",
    "    reg_alpha=1,\n",
    "    reg_lambda=5,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# we get out of fold predictions for further exploration\n",
    "submission, y_pred, y_pred_non_rounded, y_true, optimized_thresholds = TrainML(model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46638ea",
   "metadata": {
    "papermill": {
     "duration": 0.021653,
     "end_time": "2024-12-19T21:28:01.814646",
     "exception": false,
     "start_time": "2024-12-19T21:28:01.792993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we simulate changes in the scores by adding one more observation (with classes 0, 1, 2, 3 specified by `y_new`) and calculate how much better/worse the metric becomes at different values of the predictions `pred_new` compared to the prediction 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2656f30b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:01.859921Z",
     "iopub.status.busy": "2024-12-19T21:28:01.859652Z",
     "iopub.status.idle": "2024-12-19T21:28:01.943823Z",
     "shell.execute_reply": "2024-12-19T21:28:01.942949Z"
    },
    "papermill": {
     "duration": 0.107811,
     "end_time": "2024-12-19T21:28:01.945059",
     "exception": false,
     "start_time": "2024-12-19T21:28:01.837248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_new</th>\n",
       "      <th>pred_new=0</th>\n",
       "      <th>pred_new=1</th>\n",
       "      <th>pred_new=2</th>\n",
       "      <th>pred_new=3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000313</td>\n",
       "      <td>-0.000880</td>\n",
       "      <td>-0.001701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.001738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>0.003454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_new  pred_new=0  pred_new=1  pred_new=2  pred_new=3\n",
       "0      0         0.0   -0.000313   -0.000880   -0.001701\n",
       "1      1         0.0    0.000261    0.000267    0.000018\n",
       "2      2         0.0    0.000835    0.001415    0.001738\n",
       "3      3         0.0    0.001407    0.002559    0.003454"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score_changes = []\n",
    "for y_new in range(4):\n",
    "    item = {'y_new': y_new}\n",
    "    score_pred_zero = quadratic_weighted_kappa(list(y_true) + [y_new], list(y_pred) + [0])\n",
    "    for pred_new in range(4):\n",
    "        score = quadratic_weighted_kappa(list(y_true) + [y_new], list(y_pred) + [pred_new])\n",
    "        item[f'pred_new={pred_new}'] = score - score_pred_zero\n",
    "    df_score_changes.append(item)\n",
    "\n",
    "df_score_changes = pd.DataFrame(df_score_changes)\n",
    "df_score_changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f069a8",
   "metadata": {
    "papermill": {
     "duration": 0.02178,
     "end_time": "2024-12-19T21:28:01.990317",
     "exception": false,
     "start_time": "2024-12-19T21:28:01.968537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Our analysis reveals that when `y_new=2`, predicting class 3 results in a higher score than predicting the true class 2 for a one addtional observation ü§Ø"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c409a",
   "metadata": {
    "papermill": {
     "duration": 0.024348,
     "end_time": "2024-12-19T21:28:02.036629",
     "exception": false,
     "start_time": "2024-12-19T21:28:02.012281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's plot the change in the metric as we vary the threshold (while keeping the others fixed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299e9a72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:02.081191Z",
     "iopub.status.busy": "2024-12-19T21:28:02.080932Z",
     "iopub.status.idle": "2024-12-19T21:28:24.712216Z",
     "shell.execute_reply": "2024-12-19T21:28:24.711311Z"
    },
    "papermill": {
     "duration": 22.655073,
     "end_time": "2024-12-19T21:28:24.713665",
     "exception": false,
     "start_time": "2024-12-19T21:28:02.058592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_5.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_5.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_5.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for t_idx in range(3):\n",
    "    df_plot = []\n",
    "    for t in np.arange(0.0, 3.0, 0.001):\n",
    "        thresholds = copy.copy(optimized_thresholds)\n",
    "        thresholds[t_idx] = t\n",
    "        score = -evaluate_predictions(thresholds, y_true, y_pred_non_rounded)\n",
    "        df_plot.append({f't_{t_idx}': t, 'score': score})\n",
    "    \n",
    "    df_plot = pd.DataFrame(df_plot)\n",
    "    fig = px.line(df_plot, x=f't_{t_idx}', y='score', title=f't_{t_idx}')\n",
    "    fig.show(renderer='iframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc72141a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:24.760527Z",
     "iopub.status.busy": "2024-12-19T21:28:24.760236Z",
     "iopub.status.idle": "2024-12-19T21:28:24.773273Z",
     "shell.execute_reply": "2024-12-19T21:28:24.772389Z"
    },
    "papermill": {
     "duration": 0.037365,
     "end_time": "2024-12-19T21:28:24.774520",
     "exception": false,
     "start_time": "2024-12-19T21:28:24.737155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized_thresholds score: 0.4447091772741697\n",
      "another thresholds score: 0.4528701027295803\n"
     ]
    }
   ],
   "source": [
    "# The threshold optimizer in the code appears to be finding a local maximum, not the global maximum\n",
    "print('optimized_thresholds score:', -evaluate_predictions(optimized_thresholds, y_true, y_pred_non_rounded))\n",
    "print('another thresholds score:', -evaluate_predictions([0.6264773 , 0.89171596, 1.64], y_true, y_pred_non_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d34a4",
   "metadata": {
    "papermill": {
     "duration": 0.021941,
     "end_time": "2024-12-19T21:28:24.819880",
     "exception": false,
     "start_time": "2024-12-19T21:28:24.797939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Issues with the Quadratic Weighted Kappa metric\n",
    "\n",
    "* **Non-intuitive behavior:** It's been shown that predicting incorrect values can sometimes result in a better QWK score than predicting the actual values. This highlights the complexity of interpreting changes in the metric and can make it difficult to understand whether model improvements are truly meaningful.\n",
    "* **Loss of information due to discretization:** QWK requires discrete predictions, which means continuous model outputs need to be thresholded into distinct categories. This process can obscure a significant amount of information about the model's performance. For example, QWK doesn't consider how well items are ranked within each predicted class. Items on the edge of thresholds and those deep inside a category are treated equally, potentially masking important distinctions. In my opinion, a continuous metric might provide a more nuanced and informative assessment of model quality, potentially leading to better business decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0858be9",
   "metadata": {
    "papermill": {
     "duration": 0.021708,
     "end_time": "2024-12-19T21:28:24.863581",
     "exception": false,
     "start_time": "2024-12-19T21:28:24.841873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BASELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92264b9f",
   "metadata": {
    "papermill": {
     "duration": 0.022006,
     "end_time": "2024-12-19T21:28:24.907662",
     "exception": false,
     "start_time": "2024-12-19T21:28:24.885656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Issues with the following baseline (as well as in similar top-performing models on a public dataset)\n",
    "* Autoencoder for `train` and `test` dataset is fitted separately and it might lead to severely different encoding for the same data (see using `perform_autoencoder`)\n",
    "* Thresholds optimizer finds only local extremum (see using `KappaOPtimizer = minimize(...)`)\n",
    "\n",
    "Incredibly, this dubious technique gives a good score on the public leaderboard ü§∑. Let's hope that for private dataset something more correct will work better üôè."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62a4ebd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:24.953705Z",
     "iopub.status.busy": "2024-12-19T21:28:24.953485Z",
     "iopub.status.idle": "2024-12-19T21:28:29.478376Z",
     "shell.execute_reply": "2024-12-19T21:28:29.477156Z"
    },
    "papermill": {
     "duration": 4.55041,
     "end_time": "2024-12-19T21:28:29.480423",
     "exception": false,
     "start_time": "2024-12-19T21:28:24.930013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install /kaggle/input/pytorchtabnet/pytorch_tabnet-4.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1e7bd6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:29.527066Z",
     "iopub.status.busy": "2024-12-19T21:28:29.526732Z",
     "iopub.status.idle": "2024-12-19T21:28:29.542049Z",
     "shell.execute_reply": "2024-12-19T21:28:29.541154Z"
    },
    "papermill": {
     "duration": 0.039724,
     "end_time": "2024-12-19T21:28:29.543323",
     "exception": false,
     "start_time": "2024-12-19T21:28:29.503599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f63c4b41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:29.588733Z",
     "iopub.status.busy": "2024-12-19T21:28:29.588526Z",
     "iopub.status.idle": "2024-12-19T21:28:29.594380Z",
     "shell.execute_reply": "2024-12-19T21:28:29.593782Z"
    },
    "papermill": {
     "duration": 0.029932,
     "end_time": "2024-12-19T21:28:29.595501",
     "exception": false,
     "start_time": "2024-12-19T21:28:29.565569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c99e25b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:29.642101Z",
     "iopub.status.busy": "2024-12-19T21:28:29.641860Z",
     "iopub.status.idle": "2024-12-19T21:28:29.650618Z",
     "shell.execute_reply": "2024-12-19T21:28:29.649804Z"
    },
    "papermill": {
     "duration": 0.03378,
     "end_time": "2024-12-19T21:28:29.651829",
     "exception": false,
     "start_time": "2024-12-19T21:28:29.618049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f368535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:29.699403Z",
     "iopub.status.busy": "2024-12-19T21:28:29.699202Z",
     "iopub.status.idle": "2024-12-19T21:28:29.702264Z",
     "shell.execute_reply": "2024-12-19T21:28:29.701566Z"
    },
    "papermill": {
     "duration": 0.028329,
     "end_time": "2024-12-19T21:28:29.703442",
     "exception": false,
     "start_time": "2024-12-19T21:28:29.675113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962f09d",
   "metadata": {
    "papermill": {
     "duration": 0.025385,
     "end_time": "2024-12-19T21:28:29.752125",
     "exception": false,
     "start_time": "2024-12-19T21:28:29.726740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "- **Feature Selection**: The dataset contains features related to physical characteristics (e.g., BMI, Height, Weight), behavioral aspects (e.g., internet usage), and fitness data (e.g., endurance time). \n",
    "- **Categorical Feature Encoding**: Categorical features are mapped to numerical values using custom mappings for each unique category within the dataset. This ensures compatibility with machine learning algorithms that require numerical input.\n",
    "- **Time Series Aggregation**: Time series statistics (e.g., mean, standard deviation) from the actigraphy data are computed and merged into the main dataset to create additional features for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d871d42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:29.797279Z",
     "iopub.status.busy": "2024-12-19T21:28:29.797057Z",
     "iopub.status.idle": "2024-12-19T21:28:29.809424Z",
     "shell.execute_reply": "2024-12-19T21:28:29.808811Z"
    },
    "papermill": {
     "duration": 0.036487,
     "end_time": "2024-12-19T21:28:29.810685",
     "exception": false,
     "start_time": "2024-12-19T21:28:29.774198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*2, input_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*3, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    \n",
    "    input_dim = data_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i : i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n",
    "                 \n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "        \n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "def feature_engineering(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "    df['BMI_PHR'] = df['Physical-BMI'] * df['Physical-HeartRate']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7bbd37",
   "metadata": {
    "papermill": {
     "duration": 0.02201,
     "end_time": "2024-12-19T21:28:29.854947",
     "exception": false,
     "start_time": "2024-12-19T21:28:29.832937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# T√≠me series feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccd0c282",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:29.900321Z",
     "iopub.status.busy": "2024-12-19T21:28:29.900112Z",
     "iopub.status.idle": "2024-12-19T21:28:29.906860Z",
     "shell.execute_reply": "2024-12-19T21:28:29.906203Z"
    },
    "papermill": {
     "duration": 0.03094,
     "end_time": "2024-12-19T21:28:29.908044",
     "exception": false,
     "start_time": "2024-12-19T21:28:29.877104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from tqdm import tqdm \n",
    "def extract_enmo(df_source, id=None):\n",
    "    df = df_source.copy()\n",
    "    df = df[df['non-wear_flag'] == 0]\n",
    "    df.drop('non-wear_flag', axis=1, inplace=True)\n",
    "    df.loc[:, 'Type_activity'] = 'Non-assigned'\n",
    "    df.loc[(df['enmo'] < 10*1e-3), 'Type_activity'] = 'sedentary'\n",
    "    df.loc[(df['enmo'] >= 10*1e-3) & (df['enmo'] < 100*1e-3), 'Type_activity'] = 'light'\n",
    "    df.loc[(df['enmo'] >= 100*1e-3), 'Type_activity'] = 'moderate'\n",
    "    \n",
    "    total_wear = df['step'].count()\n",
    "    \n",
    "    sedentary_perall = df[df['Type_activity'] == 'sedentary']['step'].count()\n",
    "    sedentary_perall = sedentary_perall / total_wear\n",
    "    \n",
    "    light_perall = df[df['Type_activity'] == 'light']['step'].count()\n",
    "    light_perall = light_perall / total_wear\n",
    "    \n",
    "    moderate_perall = df[df['Type_activity'] == 'moderate']['step'].count()\n",
    "    moderate_perall = moderate_perall / total_wear\n",
    "\n",
    "    sedentary_perall, light_perall, moderate_perall\n",
    "    return pd.DataFrame({'id': [id], \n",
    "                         'sedentary_por': [sedentary_perall], \n",
    "                         'light_por': [light_perall],\n",
    "                         'moderate_por': [moderate_perall]}\n",
    "                       )\n",
    "\n",
    "def getEnmo(ts_path):\n",
    "    listdir = os.listdir(ts_path)\n",
    "    res_df = None\n",
    "    for dir in tqdm(listdir):\n",
    "        # print(dir)\n",
    "        dft = pd.read_parquet(os.path.join(ts_path, dir, \"part-0.parquet\"))\n",
    "        \n",
    "        id = dir[3:]\n",
    "        ex_df = extract_enmo(dft, id=id)\n",
    "        if res_df is None:\n",
    "            res_df = ex_df\n",
    "        else:\n",
    "            res_df = pd.concat([res_df, ex_df])\n",
    "    return res_df\n",
    "\n",
    "#res_df l√† k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bd39d8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:29.953200Z",
     "iopub.status.busy": "2024-12-19T21:28:29.952991Z",
     "iopub.status.idle": "2024-12-19T21:28:29.966165Z",
     "shell.execute_reply": "2024-12-19T21:28:29.965509Z"
    },
    "papermill": {
     "duration": 0.03701,
     "end_time": "2024-12-19T21:28:29.967251",
     "exception": false,
     "start_time": "2024-12-19T21:28:29.930241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cur_epoches = 0\n",
    "        #input (N, 5)\n",
    "        self.conv1 = nn.Conv1d(5, 64, kernel_size=3, stride=2, padding='valid') # 32, N, 1\n",
    "        self.avgpool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=2, padding='valid') # 32, N, 1\n",
    "        self.avgpool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(128, 128, kernel_size=3, stride=2, padding='valid') # 32, N, 1\n",
    "        self.avgpool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, stride=2, padding='valid') # 32, N, 1\n",
    "        self.avgpool4 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5 = nn.Conv1d(256, 256, kernel_size=3, stride=2, padding='valid')\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, 128) # Adjust output size based on input dims\n",
    "        self.fc15 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 4)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.3, inplace=False)\n",
    "        self.dropout2 = nn.Dropout(0.3, inplace=False)\n",
    "\n",
    "        self.act = nn.LeakyReLU(0.1)\n",
    "        self.act1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, debug=False):\n",
    "        x = self.act(self.conv1(x))\n",
    "        if debug: print(x.shape)\n",
    "        x = self.avgpool1(x)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = self.act(self.conv2(x))\n",
    "        if debug: print(x.shape)\n",
    "        x = self.avgpool2(x)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = self.act(self.conv3(x))\n",
    "        if debug: print(x.shape)\n",
    "        x = self.avgpool3(x)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = self.act(self.conv4(x))\n",
    "        if debug: print(x.shape)\n",
    "        x = self.avgpool4(x)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = self.act(self.conv5(x))\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = F.adaptive_avg_pool1d(x, 1).squeeze()\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = self.act1(self.fc1(x))\n",
    "        if debug: print(x.shape)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.act1(self.fc15(x))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        if debug: print(x.shape)\n",
    "        x = F.softmax(x, dim=0)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def feature_extract(self, x, debug=False):\n",
    "        x = self.act(self.conv1(x))\n",
    "        if debug: print(x.shape)\n",
    "        x = self.avgpool1(x)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = self.act(self.conv2(x))\n",
    "        if debug: print(x.shape)\n",
    "        x = self.avgpool2(x)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = self.act(self.conv3(x))\n",
    "        if debug: print(x.shape)\n",
    "        x = self.avgpool3(x)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = self.act(self.conv4(x))\n",
    "        if debug: print(x.shape)\n",
    "        x = self.avgpool4(x)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = self.act(self.conv5(x))\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = F.adaptive_avg_pool1d(x, 1).squeeze()\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = self.act1(self.fc1(x))\n",
    "        if debug: print(x.shape)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.act1(self.fc15(x)) #64\n",
    "        y = self.dropout2(x)\n",
    "        \n",
    "        y = self.fc2(y)\n",
    "        if debug: print(y.shape)\n",
    "        y = F.softmax(y, dim=0) #4\n",
    "\n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a180685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:30.012411Z",
     "iopub.status.busy": "2024-12-19T21:28:30.012192Z",
     "iopub.status.idle": "2024-12-19T21:28:30.021876Z",
     "shell.execute_reply": "2024-12-19T21:28:30.021064Z"
    },
    "papermill": {
     "duration": 0.033482,
     "end_time": "2024-12-19T21:28:30.023018",
     "exception": false,
     "start_time": "2024-12-19T21:28:29.989536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, device, path_tabu, path_ts, preload=True, type='train'):\n",
    "        self.device = device\n",
    "        self.path_ts = path_ts\n",
    "        self.tabu_data = pd.read_csv(path_tabu) \n",
    "        self.ids = [x[3:] for x in os.listdir(path_ts)]\n",
    "        self.filter()\n",
    "        self.ids.sort()\n",
    "        self.type = type\n",
    "        if type == 'train':\n",
    "            n = len(self.ids)\n",
    "            self.ids = self.ids[:int(n*0.8)]\n",
    "        elif type == 'val':\n",
    "            n = len(self.ids)\n",
    "            self.ids = self.ids[int(n*0.8):]\n",
    "        self.preload = preload\n",
    "        if self.preload:\n",
    "            self.ts_data_X, self.ts_data_Y = self.load_all_data()\n",
    "    def filter(self):\n",
    "        temp_ids = []\n",
    "        for id in tqdm(self.ids):\n",
    "            df = pd.read_parquet(os.path.join(self.path_ts, \"id=\" + id, \"part-0.parquet\"))\n",
    "            if df.shape[0] >= 900:\n",
    "                temp_ids.append(id)\n",
    "        self.ids = temp_ids\n",
    "\n",
    "    def collate(self, index):\n",
    "        X = [self.ts_data_X[i].to(self.device) for i in index]\n",
    "        Y = None\n",
    "        if self.ts_data_Y is not None:\n",
    "            Y = [self.ts_data_Y[i] for i in index]\n",
    "            Y = torch.tensor(Y, dtype=torch.int64)\n",
    "            Y = torch.nn.functional.one_hot(Y, num_classes=4).to(self.device).to(torch.float32)\n",
    "        return {'X': X,\n",
    "                'Y': Y}\n",
    "            \n",
    "    def dataloader(self, batch_size=1):\n",
    "        size = len(self.ts_data_X)\n",
    "        batch_size = size if batch_size == -1 else batch_size\n",
    "        loader = DataLoader(list(range(size)), batch_size=batch_size, collate_fn=self.collate, shuffle=True if self.type == 'test' else False, num_workers=0)\n",
    "        return loader\n",
    "        \n",
    "    def load_all_data(self):\n",
    "        count = 0\n",
    "        inputs = []\n",
    "        labels = None if self.type == 'test' else list()\n",
    "        for id in tqdm(self.ids):\n",
    "            df = pd.read_parquet(os.path.join(self.path_ts, \"id=\" + id, \"part-0.parquet\"))\n",
    "            df = df.loc[:, ['X', 'Y', 'Z', 'enmo', 'anglez']]\n",
    "            # normalize the signals \n",
    "            scaler = StandardScaler()\n",
    "            df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "            input = torch.tensor(df.values)\n",
    "            input = input.T\n",
    "            inputs.append(input)\n",
    "            if self.type != 'test': labels.append(self.tabu_data[self.tabu_data['id'] == id]['sii'].values[0])\n",
    "\n",
    "            \n",
    "            # count += 1\n",
    "            # if count == 2:\n",
    "            #     break\n",
    "        \n",
    "        return inputs, labels\n",
    "        \n",
    "# train_tabu_path=\"/kaggle/input/child-mind-institute-problematic-internet-use/train.csv\"\n",
    "# train_ts_path='/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet'\n",
    "# dataset = Dataset('cpu', path_tabu=train_tabu_path, path_ts=train_ts_path, type='train')\n",
    "# dataloader = dataset.dataloader()\n",
    "# model.zero_grad()\n",
    "# for data in dataloader:\n",
    "#     print(data['X'][0])\n",
    "#     print(data['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93bc2c78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:30.067500Z",
     "iopub.status.busy": "2024-12-19T21:28:30.067279Z",
     "iopub.status.idle": "2024-12-19T21:28:30.074228Z",
     "shell.execute_reply": "2024-12-19T21:28:30.073441Z"
    },
    "papermill": {
     "duration": 0.030586,
     "end_time": "2024-12-19T21:28:30.075482",
     "exception": false,
     "start_time": "2024-12-19T21:28:30.044896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature_64_4(tabu_path, ts_path, merge=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_path = \"/kaggle/input/cnn-feature_extractor/pytorch/default/5/model-50 (1).pth\"\n",
    "    model = CNN()\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    trainset = Dataset(device, path_tabu=tabu_path, path_ts=ts_path, type='test')\n",
    "    dataloader = trainset.dataloader(batch_size=1)\n",
    "\n",
    "\n",
    "    features64 = list()\n",
    "    features4 = list()\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            X = data['X'][0]\n",
    "            x_64, x_4 = model.feature_extract(X)\n",
    "            features64.append(x_64)\n",
    "            features4.append(x_4)\n",
    "    \n",
    "    features64 = torch.stack(features64, dim=0)\n",
    "    features4 = torch.stack(features4, dim=0)\n",
    "    features64 = features64.cpu().detach().numpy()\n",
    "    features4 = features4.cpu().detach().numpy()\n",
    "    ids = np.array(trainset.ids)\n",
    "    if merge:\n",
    "        features68 = np.concatenate([features64, features4], axis=-1)\n",
    "        ts_features_df = pd.DataFrame(features68, columns=[f'feature_{i}' for i in range(features68.shape[1])])\n",
    "        ts_features_df.insert(0, 'id', ids)\n",
    "        return ts_features_df\n",
    "    else:\n",
    "        ts_features_df64 = pd.DataFrame(features64, columns=[f'feature64_{i}' for i in range(features64.shape[1])])\n",
    "        ts_features_df64.insert(0, 'id', ids)\n",
    "\n",
    "        ts_features_df4 = pd.DataFrame(features4, columns=[f'feature4_{i}' for i in range(features4.shape[1])])\n",
    "        ts_features_df4.insert(0, 'id', ids)\n",
    "        return ts_features_df64, ts_features_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "700c9fbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:30.122099Z",
     "iopub.status.busy": "2024-12-19T21:28:30.121884Z",
     "iopub.status.idle": "2024-12-19T21:28:30.124682Z",
     "shell.execute_reply": "2024-12-19T21:28:30.124077Z"
    },
    "papermill": {
     "duration": 0.027136,
     "end_time": "2024-12-19T21:28:30.126018",
     "exception": false,
     "start_time": "2024-12-19T21:28:30.098882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tabu_path_train = '/kaggle/input/child-mind-institute-problematic-internet-use/train.csv'\n",
    "#ts_path_train = '/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet'\n",
    "\n",
    "#tabu_path_test = '/kaggle/input/child-mind-institute-problematic-internet-use/test.csv'\n",
    "#ts_path_test = '/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet'\n",
    "\n",
    "#train_68 = get_feature_64_4(tabu_path_train, ts_path_train, merge=True)\n",
    "#train_68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e60c86ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:30.170795Z",
     "iopub.status.busy": "2024-12-19T21:28:30.170561Z",
     "iopub.status.idle": "2024-12-19T21:28:30.173241Z",
     "shell.execute_reply": "2024-12-19T21:28:30.172536Z"
    },
    "papermill": {
     "duration": 0.026281,
     "end_time": "2024-12-19T21:28:30.174397",
     "exception": false,
     "start_time": "2024-12-19T21:28:30.148116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#l = train_64.columns.tolist()\n",
    "#l.remove('id')\n",
    "#print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18d539df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:30.220126Z",
     "iopub.status.busy": "2024-12-19T21:28:30.219927Z",
     "iopub.status.idle": "2024-12-19T21:28:30.222833Z",
     "shell.execute_reply": "2024-12-19T21:28:30.222032Z"
    },
    "papermill": {
     "duration": 0.027287,
     "end_time": "2024-12-19T21:28:30.224193",
     "exception": false,
     "start_time": "2024-12-19T21:28:30.196906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_64, train_4 = get_feature_64_4(tabu_path_train, ts_path_train, merge=False)\n",
    "#train_64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a63740a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:30.270302Z",
     "iopub.status.busy": "2024-12-19T21:28:30.270067Z",
     "iopub.status.idle": "2024-12-19T21:28:30.273241Z",
     "shell.execute_reply": "2024-12-19T21:28:30.272442Z"
    },
    "papermill": {
     "duration": 0.027085,
     "end_time": "2024-12-19T21:28:30.274360",
     "exception": false,
     "start_time": "2024-12-19T21:28:30.247275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c85d135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:30.319712Z",
     "iopub.status.busy": "2024-12-19T21:28:30.319485Z",
     "iopub.status.idle": "2024-12-19T21:28:30.322351Z",
     "shell.execute_reply": "2024-12-19T21:28:30.321562Z"
    },
    "papermill": {
     "duration": 0.026635,
     "end_time": "2024-12-19T21:28:30.323529",
     "exception": false,
     "start_time": "2024-12-19T21:28:30.296894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_enmo = getEnmo(ts_path_train)\n",
    "#test_enmo = getEnmo(ts_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e352744",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:30.368463Z",
     "iopub.status.busy": "2024-12-19T21:28:30.368241Z",
     "iopub.status.idle": "2024-12-19T21:28:30.371279Z",
     "shell.execute_reply": "2024-12-19T21:28:30.370403Z"
    },
    "papermill": {
     "duration": 0.026743,
     "end_time": "2024-12-19T21:28:30.372521",
     "exception": false,
     "start_time": "2024-12-19T21:28:30.345778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test_enmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ac76f95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:30.417578Z",
     "iopub.status.busy": "2024-12-19T21:28:30.417353Z",
     "iopub.status.idle": "2024-12-19T21:28:30.420327Z",
     "shell.execute_reply": "2024-12-19T21:28:30.419565Z"
    },
    "papermill": {
     "duration": 0.026914,
     "end_time": "2024-12-19T21:28:30.421528",
     "exception": false,
     "start_time": "2024-12-19T21:28:30.394614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_enmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8dcda6",
   "metadata": {
    "papermill": {
     "duration": 0.02217,
     "end_time": "2024-12-19T21:28:30.466398",
     "exception": false,
     "start_time": "2024-12-19T21:28:30.444228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08a5e12e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:28:30.512042Z",
     "iopub.status.busy": "2024-12-19T21:28:30.511788Z",
     "iopub.status.idle": "2024-12-19T21:31:43.548935Z",
     "shell.execute_reply": "2024-12-19T21:31:43.547957Z"
    },
    "papermill": {
     "duration": 193.06138,
     "end_time": "2024-12-19T21:31:43.550206",
     "exception": false,
     "start_time": "2024-12-19T21:28:30.488826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 996/996 [00:18<00:00, 54.02it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 996/996 [00:38<00:00, 26.14it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 54.37it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 32.41it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 996/996 [01:27<00:00, 11.33it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 22.02it/s]\n"
     ]
    }
   ],
   "source": [
    "tabu_path_train = '/kaggle/input/child-mind-institute-problematic-internet-use/train.csv'\n",
    "ts_path_train = '/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet'\n",
    "\n",
    "tabu_path_test = '/kaggle/input/child-mind-institute-problematic-internet-use/test.csv'\n",
    "ts_path_test = '/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet'\n",
    "\n",
    "train_64, train_4 = get_feature_64_4(tabu_path_train, ts_path_train, merge=False)\n",
    "test_64, test_4 = get_feature_64_4(tabu_path_test, ts_path_test, merge=False)\n",
    "\n",
    "train_enmo = getEnmo(ts_path_train)\n",
    "test_enmo = getEnmo(ts_path_test)\n",
    "\n",
    "train_ts_7 = train_enmo.merge(train_4, on ='id', how='left')\n",
    "test_ts_7 = test_enmo.merge(test_4, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2039196e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:31:43.681299Z",
     "iopub.status.busy": "2024-12-19T21:31:43.681013Z",
     "iopub.status.idle": "2024-12-19T21:33:09.753437Z",
     "shell.execute_reply": "2024-12-19T21:33:09.752740Z"
    },
    "papermill": {
     "duration": 86.140346,
     "end_time": "2024-12-19T21:33:09.754892",
     "exception": false,
     "start_time": "2024-12-19T21:31:43.614546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 996/996 [01:07<00:00, 14.86it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 14.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.5167]\n",
      "Epoch [20/100], Loss: 1.4021]\n",
      "Epoch [30/100], Loss: 1.3960]\n",
      "Epoch [40/100], Loss: 1.3791]\n",
      "Epoch [50/100], Loss: 1.3787]\n",
      "Epoch [60/100], Loss: 1.3770]\n",
      "Epoch [70/100], Loss: 1.3731]\n",
      "Epoch [80/100], Loss: 1.3716]\n",
      "Epoch [90/100], Loss: 1.3713]\n",
      "Epoch [100/100], Loss: 1.3711]\n",
      "Epoch [10/100], Loss: 1.0106]\n",
      "Epoch [20/100], Loss: 0.5540]\n",
      "Epoch [30/100], Loss: 0.4271]\n",
      "Epoch [40/100], Loss: 0.4271]\n",
      "Epoch [50/100], Loss: 0.4271]\n",
      "Epoch [60/100], Loss: 0.4271]\n",
      "Epoch [70/100], Loss: 0.4271]\n",
      "Epoch [80/100], Loss: 0.4271]\n",
      "Epoch [90/100], Loss: 0.4271]\n",
      "Epoch [100/100], Loss: 0.4271]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "df_train = train_ts.drop('id', axis=1)\n",
    "df_test = test_ts.drop('id', axis=1)\n",
    "\n",
    "train_ts_encoded = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\n",
    "test_ts_encoded = perform_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n",
    "\n",
    "train_ts_encoded[\"id\"]=train_ts[\"id\"]\n",
    "test_ts_encoded['id']=test_ts[\"id\"]\n",
    "\n",
    "train_ts_encoded = train_ts_encoded.merge(train_ts_7, on='id', how='left')\n",
    "test_ts_encoded = test_ts_encoded.merge(test_ts_7, on='id', how='left')\n",
    "\n",
    "time_series_cols = train_ts_encoded.columns.tolist()\n",
    "time_series_cols.remove('id')\n",
    "\n",
    "\n",
    "train = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\n",
    "imputed_data = imputer.fit_transform(train[numeric_cols])\n",
    "train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
    "train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n",
    "for col in train.columns:\n",
    "    if col not in numeric_cols:\n",
    "        train_imputed[col] = train[col]\n",
    "        \n",
    "train = train_imputed\n",
    "\n",
    "train = feature_engineering(train)\n",
    "train = train.dropna(thresh=10, axis=0)\n",
    "test = feature_engineering(test)\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test  = test .drop('id', axis=1)   \n",
    "\n",
    "\n",
    "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
    "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
    "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW','BMI_PHR']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
    "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
    "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW','BMI_PHR']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "test = test[featuresCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be7cd055",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:33:09.917701Z",
     "iopub.status.busy": "2024-12-19T21:33:09.917038Z",
     "iopub.status.idle": "2024-12-19T21:33:09.926938Z",
     "shell.execute_reply": "2024-12-19T21:33:09.926069Z"
    },
    "papermill": {
     "duration": 0.092059,
     "end_time": "2024-12-19T21:33:09.928280",
     "exception": false,
     "start_time": "2024-12-19T21:33:09.836221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if np.any(np.isinf(train)):\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7b1ff0",
   "metadata": {
    "papermill": {
     "duration": 0.078592,
     "end_time": "2024-12-19T21:33:10.088005",
     "exception": false,
     "start_time": "2024-12-19T21:33:10.009413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "- **Model Types**: Various models are used, including:\n",
    "  - **LightGBM**: A gradient-boosting framework known for its speed and efficiency with large datasets.\n",
    "  - **XGBoost**: Another powerful gradient-boosting model used for structured data.\n",
    "  - **CatBoost**: Optimized for categorical features without the need for extensive preprocessing.\n",
    "  - **Voting Regressor**: An ensemble model that combines the predictions of LightGBM, XGBoost, and CatBoost for better accuracy.\n",
    "- **Cross-Validation**: Stratified K-Folds cross-validation is employed to split the data into training and validation sets, ensuring balanced class distribution in each fold.\n",
    "- **Quadratic Weighted Kappa (QWK)**: The performance of the models is evaluated using QWK, which measures the agreement between predicted and actual values, taking into account the ordinal nature of the target variable.\n",
    "- **Threshold Optimization**: The `minimize` function from `scipy.optimize` is used to fine-tune decision thresholds that map continuous predictions to discrete categories (None, Mild, Moderate, Severe).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe562d94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:33:10.247306Z",
     "iopub.status.busy": "2024-12-19T21:33:10.247017Z",
     "iopub.status.idle": "2024-12-19T21:33:10.255472Z",
     "shell.execute_reply": "2024-12-19T21:33:10.254625Z"
    },
    "papermill": {
     "duration": 0.089418,
     "end_time": "2024-12-19T21:33:10.256862",
     "exception": false,
     "start_time": "2024-12-19T21:33:10.167444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TrainML(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9198bce",
   "metadata": {
    "papermill": {
     "duration": 0.078909,
     "end_time": "2024-12-19T21:33:10.415820",
     "exception": false,
     "start_time": "2024-12-19T21:33:10.336911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Hyperparameter Tuning\n",
    "\n",
    "- **LightGBM Parameters**: Hyperparameters such as `learning_rate`, `max_depth`, `num_leaves`, and `feature_fraction` are tuned to improve the performance of the LightGBM model. These parameters control the complexity of the model and its ability to generalize to new data.\n",
    "- **XGBoost and CatBoost Parameters**: Similar tuning is applied for XGBoost and CatBoost, adjusting parameters such as `n_estimators`, `max_depth`, `learning_rate`, `subsample`, and `regularization` terms (`reg_alpha`, `reg_lambda`). These help in controlling overfitting and ensuring the model's robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b40636a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:33:10.576701Z",
     "iopub.status.busy": "2024-12-19T21:33:10.576432Z",
     "iopub.status.idle": "2024-12-19T21:33:10.581150Z",
     "shell.execute_reply": "2024-12-19T21:33:10.580440Z"
    },
    "papermill": {
     "duration": 0.085918,
     "end_time": "2024-12-19T21:33:10.582442",
     "exception": false,
     "start_time": "2024-12-19T21:33:10.496524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01,  # Increased from 2.68e-06\n",
    "    'device': 'cpu'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'gpu_hist',\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    'task_type': 'GPU'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ceb278f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:33:10.742775Z",
     "iopub.status.busy": "2024-12-19T21:33:10.742523Z",
     "iopub.status.idle": "2024-12-19T21:33:10.755792Z",
     "shell.execute_reply": "2024-12-19T21:33:10.755140Z"
    },
    "papermill": {
     "duration": 0.094706,
     "end_time": "2024-12-19T21:33:10.757021",
     "exception": false,
     "start_time": "2024-12-19T21:33:10.662315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New: TabNet\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "import os\n",
    "import torch\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "\n",
    "class TabNetWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = TabNetRegressor(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.best_model_path = 'best_tabnet_model.pt'\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Handle missing values\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "        \n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "            \n",
    "        # Create internal validation set\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_imputed, \n",
    "            y, \n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train TabNet model\n",
    "        history = self.model.fit(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train.reshape(-1, 1),\n",
    "            eval_set=[(X_valid, y_valid.reshape(-1, 1))],\n",
    "            eval_name=['valid'],\n",
    "            eval_metric=['mse'],\n",
    "            max_epochs=200,\n",
    "            patience=20,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "            callbacks=[\n",
    "                TabNetPretrainedModelCheckpoint(\n",
    "                    filepath=self.best_model_path,\n",
    "                    monitor='valid_mse',\n",
    "                    mode='min',\n",
    "                    save_best_only=True,\n",
    "                    verbose=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Load the best model\n",
    "        if os.path.exists(self.best_model_path):\n",
    "            self.model.load_model(self.best_model_path)\n",
    "            os.remove(self.best_model_path)  # Remove temporary file\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        return self.model.predict(X_imputed).flatten()\n",
    "    \n",
    "    def __deepcopy__(self, memo):\n",
    "        # Add deepcopy support for scikit-learn\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, deepcopy(v, memo))\n",
    "        return result\n",
    "\n",
    "# TabNet hyperparameters\n",
    "TabNet_Params = {\n",
    "    'n_d': 64,              # Width of the decision prediction layer\n",
    "    'n_a': 64,              # Width of the attention embedding for each step\n",
    "    'n_steps': 5,           # Number of steps in the architecture\n",
    "    'gamma': 1.5,           # Coefficient for feature selection regularization\n",
    "    'n_independent': 2,     # Number of independent GLU layer in each GLU block\n",
    "    'n_shared': 2,          # Number of shared GLU layer in each GLU block\n",
    "    'lambda_sparse': 1e-4,  # Sparsity regularization\n",
    "    'optimizer_fn': torch.optim.Adam,\n",
    "    'optimizer_params': dict(lr=2e-2, weight_decay=1e-5),\n",
    "    'mask_type': 'entmax',\n",
    "    'scheduler_params': dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n",
    "    'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'verbose': 1,\n",
    "    'device_name': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "class TabNetPretrainedModelCheckpoint(Callback):\n",
    "    def __init__(self, filepath, monitor='val_loss', mode='min', \n",
    "                 save_best_only=True, verbose=1):\n",
    "        super().__init__()  # Initialize parent class\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        self.verbose = verbose\n",
    "        self.best = float('inf') if mode == 'min' else -float('inf')\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.model = self.trainer  # Use trainer itself as model\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "        \n",
    "        # Check if current metric is better than best\n",
    "        if (self.mode == 'min' and current < self.best) or \\\n",
    "           (self.mode == 'max' and current > self.best):\n",
    "            if self.verbose:\n",
    "                print(f'\\nEpoch {epoch}: {self.monitor} improved from {self.best:.4f} to {current:.4f}')\n",
    "            self.best = current\n",
    "            if self.save_best_only:\n",
    "                self.model.save_model(self.filepath)  # Save the entire model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b33634",
   "metadata": {
    "papermill": {
     "duration": 0.0787,
     "end_time": "2024-12-19T21:33:10.915576",
     "exception": false,
     "start_time": "2024-12-19T21:33:10.836876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble Learning and Submission Preparation\n",
    "\n",
    "- **Ensemble Learning**: The model uses a **Voting Regressor**, which combines the predictions from LightGBM, XGBoost, and CatBoost. This approach is beneficial as it leverages the strengths of multiple models, reducing overfitting and improving overall model performance.\n",
    "- **Out-of-Fold (OOF) Predictions**: During cross-validation, out-of-fold predictions are generated for the training set, which helps in model evaluation without data leakage.\n",
    "- **Kappa Optimizer**: The Kappa Optimizer ensures that the predicted values are as close to the actual values as possible by adjusting the thresholds used to convert raw model outputs into class labels.\n",
    "- **Test Set Predictions**: After the model is trained and thresholds are optimized, the test dataset is processed, and predictions are generated using the ensemble model. These predictions are converted into the appropriate format for submission.\n",
    "- **Submission File Creation**: The predictions are saved in a CSV file following the required format for submission (e.g., for a Kaggle competition), which includes columns like `id` and `sii` (Severity Impairment Index)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c2fee8",
   "metadata": {
    "papermill": {
     "duration": 0.079021,
     "end_time": "2024-12-19T21:33:11.076738",
     "exception": false,
     "start_time": "2024-12-19T21:33:10.997717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final Results and Performance Metrics\n",
    "\n",
    "- **Train and Validation Scores**: After training across multiple folds, the mean Quadratic Weighted Kappa (QWK) score is calculated for both the training and validation datasets, providing an indicator of model performance. \n",
    "- **Optimized QWK Score**: The final optimized QWK score after threshold tuning is displayed, showcasing the model's ability to predict the severity levels effectively.\n",
    "- **Test Predictions**: The test set predictions are evaluated, and a breakdown of the predicted severity levels (None, Mild, Moderate, Severe) is shown, along with their respective counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d89e912",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:33:11.275373Z",
     "iopub.status.busy": "2024-12-19T21:33:11.275090Z",
     "iopub.status.idle": "2024-12-19T21:33:11.285171Z",
     "shell.execute_reply": "2024-12-19T21:33:11.284315Z"
    },
    "papermill": {
     "duration": 0.12965,
     "end_time": "2024-12-19T21:33:11.286441",
     "exception": false,
     "start_time": "2024-12-19T21:33:11.156791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model instances\n",
    "Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "TabNet_Model = TabNetWrapper(**TabNet_Params) # New"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce1adb",
   "metadata": {
    "papermill": {
     "duration": 0.078354,
     "end_time": "2024-12-19T21:33:11.445121",
     "exception": false,
     "start_time": "2024-12-19T21:33:11.366767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "# **„Äã„Äã„ÄãModel1.Train**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fea90d16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:33:11.604116Z",
     "iopub.status.busy": "2024-12-19T21:33:11.603807Z",
     "iopub.status.idle": "2024-12-19T21:35:13.088195Z",
     "shell.execute_reply": "2024-12-19T21:35:13.087450Z"
    },
    "papermill": {
     "duration": 121.565005,
     "end_time": "2024-12-19T21:35:13.089541",
     "exception": false,
     "start_time": "2024-12-19T21:33:11.524536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:01<00:00, 24.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.7766\n",
      "Mean Validation QWK ---> 0.5477\n",
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.598\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    1\n",
       "1   000fd460    0\n",
       "2   00105258    1\n",
       "3   00115b9f    0\n",
       "4   0016bb22    1\n",
       "5   001f3379    1\n",
       "6   0038ba98    1\n",
       "7   0068a485    1\n",
       "8   0069fbed    1\n",
       "9   0083e397    1\n",
       "10  0087dd65    1\n",
       "11  00abe655    1\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    1\n",
       "18  00e6167c    1\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model),\n",
    "    ('tabnet', TabNet_Model)\n",
    "],weights=[4.0,4.0,5.0,4.0])\n",
    "\n",
    "Submission1 = TrainML(voting_model, test)\n",
    "\n",
    "Submission1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd879d7",
   "metadata": {
    "papermill": {
     "duration": 0.079276,
     "end_time": "2024-12-19T21:35:13.250401",
     "exception": false,
     "start_time": "2024-12-19T21:35:13.171125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```\n",
    "],weights=[5.0,4.0,4.0,4.0])\n",
    "Mean Train QWK --> 0.7424\n",
    "Mean Validation QWK ---> 0.4735\n",
    "----> || Optimized QWK SCORE ::  0.533\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b0ea2a",
   "metadata": {
    "papermill": {
     "duration": 0.078929,
     "end_time": "2024-12-19T21:35:13.411874",
     "exception": false,
     "start_time": "2024-12-19T21:35:13.332945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "# **„Äã„Äã„ÄãModel2**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64310173",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:35:13.571446Z",
     "iopub.status.busy": "2024-12-19T21:35:13.571162Z",
     "iopub.status.idle": "2024-12-19T21:37:19.061606Z",
     "shell.execute_reply": "2024-12-19T21:37:19.060724Z"
    },
    "papermill": {
     "duration": 125.571708,
     "end_time": "2024-12-19T21:37:19.062926",
     "exception": false,
     "start_time": "2024-12-19T21:35:13.491218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:57<00:00, 11.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.7591\n",
      "Mean Validation QWK ---> 0.3830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.455\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    1\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    0\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    0\n",
       "10  0087dd65    0\n",
       "11  00abe655    1\n",
       "12  00ae59c9    2\n",
       "13  00af6387    1\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "        \n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "train_ts = train_ts.merge(train_ts_7, on='id', how='left')\n",
    "test_ts = test_ts.merge(test_ts_7, on='id', how='left')\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)   \n",
    "\n",
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "        \n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train)\n",
    "    mappingTe = create_mapping(col, test)\n",
    "    \n",
    "    train[col] = train[col].replace(mapping).astype(int)\n",
    "    test[col] = test[col].replace(mappingTe).astype(int)\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def TrainML(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.49, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    thresholds = KappaOPtimizer.x\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    fold_weights = [1.25, 1.0, 1.0, 1.0, 1.0]\n",
    "    tpm = test_preds.dot(fold_weights) / np.sum(fold_weights)\n",
    "    tpTuned = threshold_Rounder(tpm, thresholds)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission\n",
    "\n",
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01  # Increased from 2.68e-06\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'cat_features': cat_c,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10  # Increase this value\n",
    "}\n",
    "\n",
    "# Create model instances\n",
    "Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "\n",
    "# Combine models using Voting Regressor\n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model)\n",
    "])\n",
    "\n",
    "# Train the ensemble model\n",
    "Submission2 = TrainML(voting_model, test)\n",
    "\n",
    "# Save submission\n",
    "#Submission2.to_csv('submission.csv', index=False)\n",
    "Submission2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3931bf",
   "metadata": {
    "papermill": {
     "duration": 0.078453,
     "end_time": "2024-12-19T21:37:19.221376",
     "exception": false,
     "start_time": "2024-12-19T21:37:19.142923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "# **„Äã„Äã„ÄãModel3**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "800a9b78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:37:19.383965Z",
     "iopub.status.busy": "2024-12-19T21:37:19.383620Z",
     "iopub.status.idle": "2024-12-19T21:40:35.047650Z",
     "shell.execute_reply": "2024-12-19T21:40:35.046726Z"
    },
    "papermill": {
     "duration": 195.748512,
     "end_time": "2024-12-19T21:40:35.048889",
     "exception": false,
     "start_time": "2024-12-19T21:37:19.300377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:07<00:00, 25.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.9189\n",
      "Mean Validation QWK ---> 0.3734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.463\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    2\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    1\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    2\n",
       "9   0083e397    0\n",
       "10  0087dd65    1\n",
       "11  00abe655    1\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    2\n",
       "15  00c0cd71    2\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "train_ts = train_ts.merge(train_ts_7, on='id', how='left')\n",
    "test_ts = test_ts.merge(test_ts_7, on='id', how='left')\n",
    "\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "\n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train)\n",
    "    mappingTe = create_mapping(col, test)\n",
    "    \n",
    "    train[col] = train[col].replace(mapping).astype(int)\n",
    "    test[col] = test[col].replace(mappingTe).astype(int)\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def TrainML(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    thresholds = KappaOPtimizer.x\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tp_rounded = threshold_Rounder(tpm, thresholds)\n",
    "\n",
    "    return tp_rounded\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "ensemble = VotingRegressor(estimators=[\n",
    "    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n",
    "    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n",
    "    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n",
    "    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n",
    "    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n",
    "])\n",
    "\n",
    "Submission3 = TrainML(ensemble, test)\n",
    "Submission3 = pd.DataFrame({\n",
    "    'id': sample['id'],\n",
    "    'sii': Submission3\n",
    "})\n",
    "\n",
    "Submission3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70c9a6",
   "metadata": {
    "papermill": {
     "duration": 0.07823,
     "end_time": "2024-12-19T21:40:35.245081",
     "exception": false,
     "start_time": "2024-12-19T21:40:35.166851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "082bd0c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:40:35.405698Z",
     "iopub.status.busy": "2024-12-19T21:40:35.405376Z",
     "iopub.status.idle": "2024-12-19T21:40:35.423507Z",
     "shell.execute_reply": "2024-12-19T21:40:35.422710Z"
    },
    "papermill": {
     "duration": 0.099536,
     "end_time": "2024-12-19T21:40:35.424870",
     "exception": false,
     "start_time": "2024-12-19T21:40:35.325334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority voting completed and saved to 'Final_Submission.csv'\n"
     ]
    }
   ],
   "source": [
    "sub1 = Submission1\n",
    "sub2 = Submission2\n",
    "sub3 = Submission3\n",
    "\n",
    "sub1 = sub1.sort_values(by='id').reset_index(drop=True)\n",
    "sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n",
    "sub3 = sub3.sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'id': sub1['id'],\n",
    "    'sii_1': sub1['sii'],\n",
    "    'sii_2': sub2['sii'],\n",
    "    'sii_3': sub3['sii']\n",
    "})\n",
    "\n",
    "def majority_vote(row):\n",
    "    return row.mode()[0]\n",
    "\n",
    "combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3']].apply(majority_vote, axis=1)\n",
    "\n",
    "final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n",
    "\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Majority voting completed and saved to 'Final_Submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "928d3c82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:40:35.583569Z",
     "iopub.status.busy": "2024-12-19T21:40:35.583291Z",
     "iopub.status.idle": "2024-12-19T21:40:35.590788Z",
     "shell.execute_reply": "2024-12-19T21:40:35.589746Z"
    },
    "papermill": {
     "duration": 0.088081,
     "end_time": "2024-12-19T21:40:35.592060",
     "exception": false,
     "start_time": "2024-12-19T21:40:35.503979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    1\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    1\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    0\n",
       "10  0087dd65    1\n",
       "11  00abe655    1\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "874d5718",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T21:40:35.753022Z",
     "iopub.status.busy": "2024-12-19T21:40:35.752704Z",
     "iopub.status.idle": "2024-12-19T21:40:35.764841Z",
     "shell.execute_reply": "2024-12-19T21:40:35.764147Z"
    },
    "papermill": {
     "duration": 0.094586,
     "end_time": "2024-12-19T21:40:35.766077",
     "exception": false,
     "start_time": "2024-12-19T21:40:35.671491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii_1</th>\n",
       "      <th>sii_2</th>\n",
       "      <th>sii_3</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii_1  sii_2  sii_3  final\n",
       "0   00008ff9      1      1      2      1\n",
       "1   000fd460      0      0      0      0\n",
       "2   00105258      1      0      0      0\n",
       "3   00115b9f      0      0      0      0\n",
       "4   0016bb22      1      0      1      1\n",
       "5   001f3379      1      1      1      1\n",
       "6   0038ba98      1      0      0      0\n",
       "7   0068a485      1      0      0      0\n",
       "8   0069fbed      1      1      2      1\n",
       "9   0083e397      1      0      0      0\n",
       "10  0087dd65      1      0      1      1\n",
       "11  00abe655      1      1      1      1\n",
       "12  00ae59c9      1      2      1      1\n",
       "13  00af6387      1      1      1      1\n",
       "14  00bd4359      1      1      2      1\n",
       "15  00c0cd71      1      1      2      1\n",
       "16  00d56d4b      0      0      0      0\n",
       "17  00d9913d      1      0      0      0\n",
       "18  00e6167c      1      0      0      0\n",
       "19  00ebc35d      1      1      1      1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1 = Submission1\n",
    "sub2 = Submission2\n",
    "sub3 = Submission3\n",
    "\n",
    "sub1 = sub1.sort_values(by='id').reset_index(drop=True)\n",
    "sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n",
    "sub3 = sub3.sort_values(by='id').reset_index(drop=True)\n",
    "final_submission = final_submission.sort_values(by='id').reset_index(drop=True)\n",
    "combined = pd.DataFrame({\n",
    "    'id': sub1['id'],\n",
    "    'sii_1': sub1['sii'],\n",
    "    'sii_2': sub2['sii'],\n",
    "    'sii_3': sub3['sii'],\n",
    "    'final': final_submission['sii']\n",
    "})\n",
    "\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd791ba1",
   "metadata": {
    "papermill": {
     "duration": 0.079248,
     "end_time": "2024-12-19T21:40:35.925160",
     "exception": false,
     "start_time": "2024-12-19T21:40:35.845912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    },
    {
     "datasetId": 921302,
     "sourceId": 7453542,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 195260,
     "modelInstanceId": 172924,
     "sourceId": 203983,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 869.488118,
   "end_time": "2024-12-19T21:40:39.883177",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-19T21:26:10.395059",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
